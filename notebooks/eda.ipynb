{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "\n",
        "This notebook contains comprehensive exploratory analysis of the credit risk data for the Bati Bank buy-now-pay-later service.\n",
        "\n",
        "## Objectives\n",
        "1. Understand the structure and quality of the dataset\n",
        "2. Identify patterns and relationships in the data\n",
        "3. Detect outliers and missing values\n",
        "4. Form hypotheses for feature engineering\n",
        "5. Generate insights to guide model development\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('../data/raw/data.csv')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXPLORATORY DATA ANALYSIS - CREDIT RISK MODEL\")\n",
        "print(\"=\"*80)\n",
        "print(f'\\nData shape: {df.shape}')\n",
        "print(f'Number of rows: {df.shape[0]:,}')\n",
        "print(f'Number of columns: {df.shape[1]}')\n",
        "print(f'\\nColumns: {list(df.columns)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Overview of the Data\n",
        "\n",
        "Understanding the structure of the dataset, including data types and basic information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"=\"*80)\n",
        "print(\"1. DATA OVERVIEW\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n--- Data Types ---\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n--- First 5 rows ---\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\n--- Last 5 rows ---\")\n",
        "display(df.tail())\n",
        "\n",
        "print(\"\\n--- Sample of data ---\")\n",
        "display(df.sample(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert TransactionStartTime to datetime\n",
        "df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'])\n",
        "\n",
        "# Extract date components for analysis\n",
        "df['Year'] = df['TransactionStartTime'].dt.year\n",
        "df['Month'] = df['TransactionStartTime'].dt.month\n",
        "df['Day'] = df['TransactionStartTime'].dt.day\n",
        "df['DayOfWeek'] = df['TransactionStartTime'].dt.dayofweek\n",
        "df['Hour'] = df['TransactionStartTime'].dt.hour\n",
        "\n",
        "print(\"TransactionStartTime converted to datetime\")\n",
        "print(f\"Date range: {df['TransactionStartTime'].min()} to {df['TransactionStartTime'].max()}\")\n",
        "print(f\"Time span: {(df['TransactionStartTime'].max() - df['TransactionStartTime'].min()).days} days\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify numerical and categorical columns\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Remove ID columns from categorical for analysis\n",
        "id_cols = ['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId']\n",
        "categorical_cols = [col for col in categorical_cols if col not in id_cols]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COLUMN CATEGORIZATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nNumerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
        "print(f\"\\nCategorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
        "print(f\"\\nID columns: {id_cols}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Summary Statistics\n",
        "\n",
        "Understanding the central tendency, dispersion, and shape of the dataset's distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics for numerical features\n",
        "print(\"=\"*80)\n",
        "print(\"2. SUMMARY STATISTICS - NUMERICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "display(df[numerical_cols].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional statistics for key numerical features\n",
        "print(\"\\n--- Additional Statistics for Key Features ---\")\n",
        "key_features = ['Amount', 'Value', 'FraudResult']\n",
        "\n",
        "for feature in key_features:\n",
        "    if feature in df.columns:\n",
        "        print(f\"\\n{feature}:\")\n",
        "        print(f\"  Mean: {df[feature].mean():.2f}\")\n",
        "        print(f\"  Median: {df[feature].median():.2f}\")\n",
        "        print(f\"  Std Dev: {df[feature].std():.2f}\")\n",
        "        print(f\"  Min: {df[feature].min():.2f}\")\n",
        "        print(f\"  Max: {df[feature].max():.2f}\")\n",
        "        print(f\"  Skewness: {df[feature].skew():.2f}\")\n",
        "        print(f\"  Kurtosis: {df[feature].kurtosis():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics for categorical features\n",
        "print(\"=\"*80)\n",
        "print(\"2. SUMMARY STATISTICS - CATEGORICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\n--- {col} ---\")\n",
        "    print(f\"Unique values: {df[col].nunique()}\")\n",
        "    print(f\"Most frequent: {df[col].mode()[0] if len(df[col].mode()) > 0 else 'N/A'}\")\n",
        "    print(f\"Frequency of most frequent: {df[col].value_counts().iloc[0] if len(df[col].value_counts()) > 0 else 0}\")\n",
        "    print(\"\\nTop 10 values:\")\n",
        "    print(df[col].value_counts().head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Distribution of Numerical Features\n",
        "\n",
        "Visualizing the distribution of numerical features to identify patterns, skewness, and potential outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution plots for key numerical features\n",
        "print(\"=\"*80)\n",
        "print(\"3. DISTRIBUTION OF NUMERICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select key numerical features for visualization (excluding ID-like columns)\n",
        "key_numerical = ['Amount', 'Value', 'CountryCode', 'PricingStrategy', 'FraudResult', 'Year', 'Month', 'DayOfWeek', 'Hour']\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, col in enumerate(key_numerical):\n",
        "    if col in df.columns:\n",
        "        ax = axes[idx]\n",
        "        df[col].hist(bins=50, ax=ax, edgecolor='black')\n",
        "        ax.set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
        "        ax.set_xlabel(col)\n",
        "        ax.set_ylabel('Frequency')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed distribution for Amount and Value (log scale for better visualization)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Amount distribution\n",
        "axes[0, 0].hist(df['Amount'], bins=100, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].set_title('Distribution of Amount (Linear Scale)', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Amount')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].axvline(df['Amount'].mean(), color='r', linestyle='--', label=f'Mean: {df[\"Amount\"].mean():.2f}')\n",
        "axes[0, 0].axvline(df['Amount'].median(), color='g', linestyle='--', label=f'Median: {df[\"Amount\"].median():.2f}')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Amount distribution (log scale for positive values)\n",
        "positive_amounts = df[df['Amount'] > 0]['Amount']\n",
        "axes[0, 1].hist(np.log10(positive_amounts + 1), bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].set_title('Distribution of Positive Amounts (Log10 Scale)', fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Log10(Amount + 1)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Value distribution\n",
        "axes[1, 0].hist(df['Value'], bins=100, edgecolor='black', alpha=0.7)\n",
        "axes[1, 0].set_title('Distribution of Value (Linear Scale)', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Value')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].axvline(df['Value'].mean(), color='r', linestyle='--', label=f'Mean: {df[\"Value\"].mean():.2f}')\n",
        "axes[1, 0].axvline(df['Value'].median(), color='g', linestyle='--', label=f'Median: {df[\"Value\"].median():.2f}')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Value distribution (log scale)\n",
        "axes[1, 1].hist(np.log10(df['Value'] + 1), bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].set_title('Distribution of Value (Log10 Scale)', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Log10(Value + 1)')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAmount Statistics:\")\n",
        "print(f\"  Positive transactions: {(df['Amount'] > 0).sum():,} ({(df['Amount'] > 0).mean()*100:.2f}%)\")\n",
        "print(f\"  Negative transactions: {(df['Amount'] < 0).sum():,} ({(df['Amount'] < 0).mean()*100:.2f}%)\")\n",
        "print(f\"  Zero transactions: {(df['Amount'] == 0).sum():,} ({(df['Amount'] == 0).mean()*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Distribution of Categorical Features\n",
        "\n",
        "Analyzing the distribution of categorical features to understand frequency and variability of categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of categorical features\n",
        "print(\"=\"*80)\n",
        "print(\"4. DISTRIBUTION OF CATEGORICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Plot distributions for key categorical features\n",
        "key_categorical = ['CurrencyCode', 'ProductCategory', 'ChannelId', 'ProviderId']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, col in enumerate(key_categorical):\n",
        "    if col in df.columns:\n",
        "        ax = axes[idx]\n",
        "        value_counts = df[col].value_counts()\n",
        "        \n",
        "        # Plot top 15 categories if there are many\n",
        "        if len(value_counts) > 15:\n",
        "            top_values = value_counts.head(15)\n",
        "            top_values.plot(kind='bar', ax=ax, color='steelblue', edgecolor='black')\n",
        "            ax.set_title(f'Top 15 {col} Distribution', fontsize=12, fontweight='bold')\n",
        "        else:\n",
        "            value_counts.plot(kind='bar', ax=ax, color='steelblue', edgecolor='black')\n",
        "            ax.set_title(f'{col} Distribution', fontsize=12, fontweight='bold')\n",
        "        \n",
        "        ax.set_xlabel(col)\n",
        "        ax.set_ylabel('Frequency')\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ProductCategory distribution with percentages\n",
        "print(\"\\n--- ProductCategory Distribution ---\")\n",
        "product_dist = df['ProductCategory'].value_counts()\n",
        "product_pct = df['ProductCategory'].value_counts(normalize=True) * 100\n",
        "\n",
        "product_df = pd.DataFrame({\n",
        "    'Count': product_dist,\n",
        "    'Percentage': product_pct\n",
        "})\n",
        "display(product_df)\n",
        "\n",
        "# Visualize ProductCategory\n",
        "plt.figure(figsize=(12, 6))\n",
        "product_dist.plot(kind='bar', color='coral', edgecolor='black')\n",
        "plt.title('ProductCategory Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Product Category')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ChannelId distribution\n",
        "print(\"\\n--- ChannelId Distribution ---\")\n",
        "channel_dist = df['ChannelId'].value_counts()\n",
        "channel_pct = df['ChannelId'].value_counts(normalize=True) * 100\n",
        "\n",
        "channel_df = pd.DataFrame({\n",
        "    'Count': channel_dist,\n",
        "    'Percentage': channel_pct\n",
        "})\n",
        "display(channel_df)\n",
        "\n",
        "# Visualize ChannelId\n",
        "plt.figure(figsize=(10, 6))\n",
        "channel_dist.plot(kind='bar', color='lightgreen', edgecolor='black')\n",
        "plt.title('ChannelId Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Channel ID')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Correlation Analysis\n",
        "\n",
        "Understanding the relationship between numerical features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix for numerical features\n",
        "print(\"=\"*80)\n",
        "print(\"5. CORRELATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select numerical features for correlation (excluding derived date features for now)\n",
        "corr_features = ['Amount', 'Value', 'CountryCode', 'PricingStrategy', 'FraudResult']\n",
        "corr_matrix = df[corr_features].corr()\n",
        "\n",
        "print(\"\\n--- Correlation Matrix ---\")\n",
        "display(corr_matrix)\n",
        "\n",
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix of Numerical Features', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Relationship between Amount and Value\n",
        "print(\"\\n--- Amount vs Value Relationship ---\")\n",
        "print(f\"Correlation between Amount and Value: {df['Amount'].abs().corr(df['Value']):.4f}\")\n",
        "print(f\"Note: Value should be absolute value of Amount\")\n",
        "\n",
        "# Check if Value is indeed absolute of Amount\n",
        "df['Amount_Abs'] = df['Amount'].abs()\n",
        "value_match = (df['Value'] == df['Amount_Abs']).all()\n",
        "print(f\"Value equals absolute Amount: {value_match}\")\n",
        "\n",
        "if not value_match:\n",
        "    mismatch_count = (df['Value'] != df['Amount_Abs']).sum()\n",
        "    print(f\"Number of mismatches: {mismatch_count} ({(mismatch_count/len(df)*100):.2f}%)\")\n",
        "\n",
        "# Scatter plot of Amount vs Value\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['Amount'].abs(), df['Value'], alpha=0.5, s=1)\n",
        "plt.plot([df['Amount'].abs().min(), df['Amount'].abs().max()], \n",
        "         [df['Amount'].abs().min(), df['Amount'].abs().max()], \n",
        "         'r--', linewidth=2, label='Perfect correlation line')\n",
        "plt.xlabel('Absolute Amount')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Amount (Absolute) vs Value', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Identifying Missing Values\n",
        "\n",
        "Identifying missing values to determine missing data patterns and decide on appropriate imputation strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"=\"*80)\n",
        "print(\"6. MISSING VALUES ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "missing_values = df.isnull().sum()\n",
        "missing_pct = (missing_values / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_values,\n",
        "    'Missing Percentage': missing_pct\n",
        "})\n",
        "\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(\"\\n--- Columns with Missing Values ---\")\n",
        "    display(missing_df)\n",
        "else:\n",
        "    print(\"\\n✓ No missing values found in the dataset!\")\n",
        "\n",
        "# Check for empty strings or whitespace\n",
        "print(\"\\n--- Checking for Empty Strings ---\")\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    empty_count = (df[col].astype(str).str.strip() == '').sum()\n",
        "    if empty_count > 0:\n",
        "        print(f\"{col}: {empty_count} empty strings ({(empty_count/len(df)*100):.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Outlier Detection\n",
        "\n",
        "Using box plots and statistical methods to identify outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plots for numerical features to detect outliers\n",
        "print(\"=\"*80)\n",
        "print(\"7. OUTLIER DETECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Box plots for key numerical features\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Amount box plot\n",
        "axes[0, 0].boxplot(df['Amount'], vert=True)\n",
        "axes[0, 0].set_title('Box Plot: Amount', fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Amount')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Value box plot\n",
        "axes[0, 1].boxplot(df['Value'], vert=True)\n",
        "axes[0, 1].set_title('Box Plot: Value', fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Value')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Log scale box plots for better visualization\n",
        "axes[1, 0].boxplot(np.log10(df['Amount'].abs() + 1), vert=True)\n",
        "axes[1, 0].set_title('Box Plot: Log10(Absolute Amount + 1)', fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Log10(Amount + 1)')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].boxplot(np.log10(df['Value'] + 1), vert=True)\n",
        "axes[1, 1].set_title('Box Plot: Log10(Value + 1)', fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Log10(Value + 1)')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical outlier detection using IQR method\n",
        "def detect_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound, Q1, Q3, IQR\n",
        "\n",
        "print(\"\\n--- Outlier Detection using IQR Method ---\")\n",
        "for col in ['Amount', 'Value']:\n",
        "    outliers, lower, upper, Q1, Q3, IQR = detect_outliers_iqr(df, col)\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Q1: {Q1:.2f}\")\n",
        "    print(f\"  Q3: {Q3:.2f}\")\n",
        "    print(f\"  IQR: {IQR:.2f}\")\n",
        "    print(f\"  Lower bound: {lower:.2f}\")\n",
        "    print(f\"  Upper bound: {upper:.2f}\")\n",
        "    print(f\"  Number of outliers: {len(outliers):,} ({(len(outliers)/len(df)*100):.2f}%)\")\n",
        "    print(f\"  Outlier range: [{outliers[col].min():.2f}, {outliers[col].max():.2f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze FraudResult distribution\n",
        "print(\"\\n--- FraudResult Analysis ---\")\n",
        "fraud_dist = df['FraudResult'].value_counts()\n",
        "fraud_pct = df['FraudResult'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(f\"Fraud cases (1): {fraud_dist.get(1, 0):,} ({fraud_pct.get(1, 0):.2f}%)\")\n",
        "print(f\"Non-fraud cases (0): {fraud_dist.get(0, 0):,} ({fraud_pct.get(0, 0):.2f}%)\")\n",
        "print(f\"\\nClass imbalance ratio: {fraud_dist.get(0, 1) / fraud_dist.get(1, 1):.2f}:1\")\n",
        "\n",
        "# Visualize FraudResult\n",
        "plt.figure(figsize=(8, 6))\n",
        "fraud_dist.plot(kind='bar', color=['green', 'red'], edgecolor='black')\n",
        "plt.title('FraudResult Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Fraud Result (0=No, 1=Yes)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze fraud by different categories\n",
        "print(\"\\n--- Fraud Analysis by Category ---\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Fraud by ProductCategory\n",
        "fraud_by_category = df.groupby('ProductCategory')['FraudResult'].agg(['sum', 'count'])\n",
        "fraud_by_category['fraud_rate'] = (fraud_by_category['sum'] / fraud_by_category['count']) * 100\n",
        "fraud_by_category = fraud_by_category.sort_values('fraud_rate', ascending=False)\n",
        "\n",
        "axes[0, 0].barh(fraud_by_category.index[:10], fraud_by_category['fraud_rate'][:10], color='coral')\n",
        "axes[0, 0].set_title('Fraud Rate by ProductCategory (Top 10)', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Fraud Rate (%)')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Fraud by ChannelId\n",
        "fraud_by_channel = df.groupby('ChannelId')['FraudResult'].agg(['sum', 'count'])\n",
        "fraud_by_channel['fraud_rate'] = (fraud_by_channel['sum'] / fraud_by_channel['count']) * 100\n",
        "fraud_by_channel = fraud_by_channel.sort_values('fraud_rate', ascending=False)\n",
        "\n",
        "axes[0, 1].barh(fraud_by_channel.index, fraud_by_channel['fraud_rate'], color='steelblue')\n",
        "axes[0, 1].set_title('Fraud Rate by ChannelId', fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Fraud Rate (%)')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Fraud by PricingStrategy\n",
        "fraud_by_pricing = df.groupby('PricingStrategy')['FraudResult'].agg(['sum', 'count'])\n",
        "fraud_by_pricing['fraud_rate'] = (fraud_by_pricing['sum'] / fraud_by_pricing['count']) * 100\n",
        "fraud_by_pricing = fraud_by_pricing.sort_values('fraud_rate', ascending=False)\n",
        "\n",
        "axes[1, 0].bar(fraud_by_pricing.index.astype(str), fraud_by_pricing['fraud_rate'], color='lightgreen', edgecolor='black')\n",
        "axes[1, 0].set_title('Fraud Rate by PricingStrategy', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Pricing Strategy')\n",
        "axes[1, 0].set_ylabel('Fraud Rate (%)')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Fraud over time (by month)\n",
        "df['YearMonth'] = df['TransactionStartTime'].dt.to_period('M')\n",
        "fraud_by_month = df.groupby('YearMonth')['FraudResult'].agg(['sum', 'count'])\n",
        "fraud_by_month['fraud_rate'] = (fraud_by_month['sum'] / fraud_by_month['count']) * 100\n",
        "\n",
        "axes[1, 1].plot(fraud_by_month.index.astype(str), fraud_by_month['fraud_rate'], marker='o', linewidth=2, markersize=6)\n",
        "axes[1, 1].set_title('Fraud Rate Over Time (by Month)', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Year-Month')\n",
        "axes[1, 1].set_ylabel('Fraud Rate (%)')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customer-level analysis (preview for feature engineering)\n",
        "print(\"\\n--- Customer-Level Statistics Preview ---\")\n",
        "customer_stats = df.groupby('CustomerId').agg({\n",
        "    'TransactionId': 'count',\n",
        "    'Amount': ['sum', 'mean', 'std'],\n",
        "    'Value': ['sum', 'mean'],\n",
        "    'FraudResult': 'sum',\n",
        "    'ProductCategory': lambda x: x.nunique(),\n",
        "    'ChannelId': lambda x: x.nunique()\n",
        "}).round(2)\n",
        "\n",
        "customer_stats.columns = ['Transaction_Count', 'Total_Amount', 'Avg_Amount', 'Std_Amount',\n",
        "                          'Total_Value', 'Avg_Value', 'Fraud_Count', 'Unique_Categories', 'Unique_Channels']\n",
        "\n",
        "print(f\"Number of unique customers: {df['CustomerId'].nunique():,}\")\n",
        "print(f\"Average transactions per customer: {customer_stats['Transaction_Count'].mean():.2f}\")\n",
        "print(f\"Median transactions per customer: {customer_stats['Transaction_Count'].median():.2f}\")\n",
        "\n",
        "display(customer_stats.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Key Insights Summary\n",
        "\n",
        "Based on the exploratory data analysis, here are the most important insights that will guide feature engineering and model development.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insight 1: Severe Class Imbalance in Fraud Detection\n",
        "- **Finding**: The dataset shows extreme class imbalance with fraud cases representing only ~0.2% of all transactions.\n",
        "- **Implication**: This will require special handling during model training (e.g., class weighting, SMOTE, or stratified sampling).\n",
        "- **Action**: Consider using techniques like stratified k-fold cross-validation and appropriate evaluation metrics (AUC-ROC, Precision-Recall curve) rather than accuracy.\n",
        "\n",
        "### Insight 2: Highly Skewed Transaction Amounts with Significant Outliers\n",
        "- **Finding**: Transaction amounts (Amount and Value) are highly right-skewed with a wide range (from small values to millions).\n",
        "- **Implication**: \n",
        "  - Standard scaling may not be sufficient; log transformation or robust scaling may be needed.\n",
        "  - Outliers may represent legitimate high-value transactions or errors that need investigation.\n",
        "- **Action**: \n",
        "  - Apply log transformation or robust scaling for Amount/Value features.\n",
        "  - Consider capping extreme values or using percentile-based features.\n",
        "  - Create features like transaction amount categories (low/medium/high).\n",
        "\n",
        "### Insight 3: Strong Relationship Between Amount and Value\n",
        "- **Finding**: Value appears to be the absolute value of Amount (with some potential discrepancies to investigate).\n",
        "- **Implication**: These features are highly correlated and may provide redundant information.\n",
        "- **Action**: \n",
        "  - Use either Amount or Value, not both, or create derived features like transaction direction (credit/debit).\n",
        "  - Investigate cases where Value ≠ |Amount| as they may indicate data quality issues.\n",
        "\n",
        "### Insight 4: Temporal Patterns and Customer Behavior Diversity\n",
        "- **Finding**: \n",
        "  - Transactions span multiple months, allowing for temporal feature engineering.\n",
        "  - Customers show diverse behavior in terms of transaction frequency, product categories, and channels used.\n",
        "- **Implication**: \n",
        "  - RFM (Recency, Frequency, Monetary) features can be engineered at the customer level.\n",
        "  - Time-based features (day of week, hour, month) may capture behavioral patterns.\n",
        "- **Action**: \n",
        "  - Create customer-level aggregations: transaction count, total/avg amount, recency of last transaction.\n",
        "  - Engineer temporal features: transaction frequency, days since first/last transaction.\n",
        "  - Build features for product category diversity and channel preferences.\n",
        "\n",
        "### Insight 5: Fraud Patterns Vary by Product Category and Channel\n",
        "- **Finding**: Different product categories and channels show varying fraud rates.\n",
        "- **Implication**: \n",
        "  - ProductCategory and ChannelId are important predictors of fraud risk.\n",
        "  - These categorical features should be encoded (one-hot, target encoding, or WoE) for model use.\n",
        "- **Action**: \n",
        "  - Create fraud rate features by category/channel (target encoding).\n",
        "  - Consider interaction features between ProductCategory, ChannelId, and transaction amounts.\n",
        "  - Use Weight of Evidence (WoE) transformation for these categorical features if using logistic regression.\n",
        "\n",
        "### Additional Considerations:\n",
        "1. **Data Quality**: No missing values detected, but need to verify Value = |Amount| relationship.\n",
        "2. **Feature Engineering Opportunities**: \n",
        "   - Customer-level RFM features (Recency, Frequency, Monetary)\n",
        "   - Transaction velocity features (transactions per day/week)\n",
        "   - Behavioral diversity features (number of unique categories, channels, providers)\n",
        "   - Temporal features (time since last transaction, transaction patterns)\n",
        "3. **Model Considerations**: \n",
        "   - Given class imbalance, focus on precision-recall metrics\n",
        "   - Consider ensemble methods or cost-sensitive learning\n",
        "   - Ensure interpretability for regulatory compliance (Basel II requirements)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary statistics\n",
        "print(\"=\"*80)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nDataset Overview:\")\n",
        "print(f\"  Total transactions: {len(df):,}\")\n",
        "print(f\"  Unique customers: {df['CustomerId'].nunique():,}\")\n",
        "print(f\"  Unique accounts: {df['AccountId'].nunique():,}\")\n",
        "print(f\"  Date range: {df['TransactionStartTime'].min()} to {df['TransactionStartTime'].max()}\")\n",
        "print(f\"  Time span: {(df['TransactionStartTime'].max() - df['TransactionStartTime'].min()).days} days\")\n",
        "print(f\"\\nTarget Variable (FraudResult):\")\n",
        "print(f\"  Fraud cases: {(df['FraudResult'] == 1).sum():,} ({(df['FraudResult'] == 1).mean()*100:.2f}%)\")\n",
        "print(f\"  Non-fraud cases: {(df['FraudResult'] == 0).sum():,} ({(df['FraudResult'] == 0).mean()*100:.2f}%)\")\n",
        "print(f\"\\nTransaction Amounts:\")\n",
        "print(f\"  Total amount: {df['Amount'].sum():,.2f}\")\n",
        "print(f\"  Average amount: {df['Amount'].mean():.2f}\")\n",
        "print(f\"  Median amount: {df['Amount'].median():.2f}\")\n",
        "print(f\"  Total value: {df['Value'].sum():,.2f}\")\n",
        "print(f\"\\nData Quality:\")\n",
        "print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"  Duplicate transactions: {df.duplicated().sum()}\")\n",
        "print(f\"\\nReady for feature engineering!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
